{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled10.ipynb","provenance":[],"authorship_tag":"ABX9TyMRnkyXGRciSkxMtZfNMeis"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Moa6z0zbgsCt"},"source":["# Let us read the libraries required and read the data.\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","# To ignore warnings, we will import another library called warnings.\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","# Let us now read the data\n","\n","iris = pd.read_csv(\"iris.csv\")\n","iris.head()\n","\n","# Now we will put labels to the class as 0,1 and 2.\n","\n","iris['Species']. replace (['setosa', 'virginica', 'versicolor'], [0, 1, 2], inplace=True)\n","\n","# For one hot encoding, we define the following function.\n","\n","def to_one_hot(Y):\n","    n_col = np.amax(Y) + 1\n","    binarized = np.zeros((len(Y), n_col))\n","    for i in range(len(Y)):\n","        binarized [i, Y[i]] = 1.\n","    return binarized\n","Let us now define a sigmoid function\n","def sigmoid_func(x):\n","    return 1/(1+np.exp(-x))\n","def sigmoid_derivative(x):\n","    return sigmoid_func(x)*(1 – sigmoid_func(x))\n","\n","# Now we will define a function for normalization\n","\n","def normalize (X, axis=-1, order=2):\n","    l2 = np. atleast_1d (np.linalg.norm(X, order, axis))\n","    l2[l2 == 0] = 1\n","    return X / np.expand_dims(l2, axis)\n","\n","# Now we will apply normalization to the features and one hot encoding to the output\n","\n","columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n","x = pd.DataFrame(iris, columns=columns)\n","x = normalize(x.as_matrix())\n","columns = ['Species']\n","y = pd.DataFrame(iris, columns=columns)\n","y = y.as_matrix()\n","y = y.flatten()\n","y = to_one_hot(y)\n","\n","# Now it’s time to apply back propagation. To do that, we need to define weights and \n","# a learning rate. Let us do that. But before that we need to split the data for training \n","# and testing.\n","\n","#Split data to training and validation data\n","X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\n","#Weights\n","w0 = 2*np.random.random((4, 5)) - 1 #for input   - 4 inputs, 3 outputs\n","w1 = 2*np.random.random((5, 3)) - 1 #for layer 1 - 5 inputs, 3 outputs\n","#learning rate\n","n = 0.1\n","\n","\n","# We will set a list for errors and see how the change in training decreases the error\n","#  via visualization.\n","\n","errors = []\n","\n","# Let us perform the feed forward and back propagation network. For backpropagation, \n","# we will use gradient descent.\n","\n","for i in range (100000):\n","\n","Feed forward network\n","\n","    layer0 = X_train\n","    layer1 = sigmoid_func(np.dot(layer0, w0))\n","    layer2 = sigmoid_func(np.dot(layer1, w1))\n","    Back propagation using gradient descent\n","    layer2_error = y_train - layer2\n","    layer2_delta = layer2_error * sigmoid_derivative(layer2)\n","    layer1_error = layer2_delta.dot (w1.T)\n","    layer1_delta = layer1_error * sigmoid_derivative(layer1)\n","    w1 += layer1.T.dot(layer2_delta) * n\n","    w0 += layer0.T.dot(layer1_delta) * n\n","    error = np.mean(np.abs(layer2_error))\n","    errors.append(error)\n","\n","# Accuracy will be gathered and visualized by subtracting the error from the training\n","#  data \n","\n","accuracy_training = (1 - error) * 100\n","\n","# Now let us visualize how accuracy increases by decreasing the error\n","\n","plt.plot(errors)\n","plt.xlabel('Training')\n","plt.ylabel('Error')\n","plt.show()\n","\n","# Let us look at the accuracy now\n","\n","print (\"Training Accuracy of the model   \" + str (round(accuracy_training,2)) + \"%\")\n","# Output: Training Accuracy of the model 99.04%\n","\n","# Our training model is performing really well. Now let us see the validation accuracy.\n","\n","#Validate\n","layer0 = X_test\n","layer1 = sigmoid_func(np.dot(layer0, w0))\n","layer2 = sigmoid_func(np.dot(layer1, w1))\n","layer2_error = y_test - layer2\n","error = np.mean(np.abs(layer2_error))\n","accuracy_validation = (1 - error) * 100\n","print (\"Validation Accuracy of the model “+ str(round(accuracy_validation,2)) + \"%\")\n","# Output: Validation Accuracy 92.86%\n"],"execution_count":null,"outputs":[]}]}